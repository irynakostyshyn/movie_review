{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie rewievs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import SpatialDropout1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_neg = pd.read_csv(\"./movie_reviews/movie_reviews/train/neg_rating.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pos = pd.read_csv(\"./movie_reviews/movie_reviews/train/pos_rating.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the GloVe corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "f = open('./glove.6B/glove.6B.300d.txt')\n",
    "for line in f:\n",
    "    values = line.split(' ')\n",
    "    word = values[0] \n",
    "    coefs = np.asarray(values[1:], dtype='float32') \n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(data, sentiment):\n",
    "    train = []\n",
    "    for i  in range(data.shape[0]):\n",
    "        file = open(\"./movie_reviews/movie_reviews/train/{0}/{1}.txt\".format(sentiment,i), \"r\")\n",
    "        train.append(file.readlines())\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = data_loader(data_neg, \"neg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_pos = data_loader(data_pos, \"pos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = d+d_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ira/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "def removing_stop_words(data):\n",
    "    lines_without_stopwords=[] \n",
    "    for seq in data: \n",
    "        seq = seq[0].lower()\n",
    "        seq_by_words = re.findall(r'(?:\\w+)', seq, flags = re.UNICODE) \n",
    "        new_line=[]\n",
    "        for word in seq_by_words:\n",
    "            if word not in stop:\n",
    "                new_line.append(word)\n",
    "        lines_without_stopwords.append(new_line)\n",
    "\n",
    "    return lines_without_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = removing_stop_words(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "MAX_NUM_WORDS = 1000\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "def prep_with_tokenize_pad(texts):\n",
    "   \n",
    "    tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "    data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "   \n",
    "    \n",
    "    print(\"data shape: \", data.shape)\n",
    "\n",
    "    return word_index, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 76906 unique tokens.\n",
      "data shape:  (25000, 100)\n"
     ]
    }
   ],
   "source": [
    "word_index_data, data = prep_with_tokenize_pad(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [0 if i < 12500 else 1 for i in range(data_neg.shape[0]+data_pos.shape[0]) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12500"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.initializers import Constant\n",
    "def embed(word_index, data):\n",
    "    EMBEDDING_DIM = embeddings_index.get('a').shape[0]\n",
    "    num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
    "    embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        if i > MAX_NUM_WORDS:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word) \n",
    "        if embedding_vector is not None:\n",
    "\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix, num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#as we have fake labels for train I should split it on test and train by my own\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix_t, num_words_t = embed(word_index_data, train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_words, 300, input_length=400, weights= [embedding_matrix], trainable=False))\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv1D(64, 5, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=4))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/6\n",
      "18000/18000 [==============================] - 65s 4ms/step - loss: 0.2641 - acc: 0.8909 - val_loss: 0.3017 - val_acc: 0.8710\n",
      "Epoch 2/6\n",
      "18000/18000 [==============================] - 65s 4ms/step - loss: 0.2168 - acc: 0.9145 - val_loss: 0.2861 - val_acc: 0.8740\n",
      "Epoch 3/6\n",
      "18000/18000 [==============================] - 65s 4ms/step - loss: 0.1676 - acc: 0.9348 - val_loss: 0.2979 - val_acc: 0.8730\n",
      "Epoch 4/6\n",
      "18000/18000 [==============================] - 65s 4ms/step - loss: 0.1355 - acc: 0.9476 - val_loss: 0.2883 - val_acc: 0.8820\n",
      "Epoch 5/6\n",
      "18000/18000 [==============================] - 65s 4ms/step - loss: 0.1123 - acc: 0.9579 - val_loss: 0.3130 - val_acc: 0.8870\n",
      "Epoch 6/6\n",
      "18000/18000 [==============================] - 65s 4ms/step - loss: 0.0843 - acc: 0.9667 - val_loss: 0.3938 - val_acc: 0.8705\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8e21075c50>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train, np.array(y_train), validation_split=0.1, epochs = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.10220872561708093, 0.9625]\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(train, y_train, verbose=0)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.40757103457450866, 0.8658]\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(test, y_test, verbose=0)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rating prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hot labels encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    " from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cocn = pd.concat([data_neg, data_pos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_one_hot():\n",
    "    dummies = pd.get_dummies(data_cocn['rating'], prefix = None)\n",
    "    return dummies.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = convert_to_one_hot()\n",
    "train_m, test_m, y_train_m, y_test_m = train_test_split(data, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix_m, num_words_m = embed(word_index_data, train_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18000/18000 [==============================] - 27s 2ms/step - loss: 1.8603 - acc: 0.3048 - val_loss: 1.7160 - val_acc: 0.3500\n",
      "Epoch 2/20\n",
      "18000/18000 [==============================] - 26s 1ms/step - loss: 1.6834 - acc: 0.3690 - val_loss: 1.6169 - val_acc: 0.3790\n",
      "Epoch 3/20\n",
      "18000/18000 [==============================] - 26s 1ms/step - loss: 1.5959 - acc: 0.3989 - val_loss: 1.5662 - val_acc: 0.3965\n",
      "Epoch 4/20\n",
      "18000/18000 [==============================] - 26s 1ms/step - loss: 1.5457 - acc: 0.4169 - val_loss: 1.5415 - val_acc: 0.4110\n",
      "Epoch 5/20\n",
      "18000/18000 [==============================] - 27s 1ms/step - loss: 1.5048 - acc: 0.4358 - val_loss: 1.5418 - val_acc: 0.4205\n",
      "Epoch 6/20\n",
      "18000/18000 [==============================] - 27s 1ms/step - loss: 1.4665 - acc: 0.4503 - val_loss: 1.5068 - val_acc: 0.4155\n",
      "Epoch 7/20\n",
      "18000/18000 [==============================] - 26s 1ms/step - loss: 1.4390 - acc: 0.4622 - val_loss: 1.5121 - val_acc: 0.4275\n",
      "Epoch 8/20\n",
      "18000/18000 [==============================] - 26s 1ms/step - loss: 1.4016 - acc: 0.4751 - val_loss: 1.5144 - val_acc: 0.4215\n",
      "Epoch 9/20\n",
      "18000/18000 [==============================] - 27s 1ms/step - loss: 1.3779 - acc: 0.4835 - val_loss: 1.5136 - val_acc: 0.4295\n",
      "Epoch 10/20\n",
      "18000/18000 [==============================] - 27s 1ms/step - loss: 1.3514 - acc: 0.4942 - val_loss: 1.5161 - val_acc: 0.4310\n",
      "Epoch 11/20\n",
      "18000/18000 [==============================] - 27s 1ms/step - loss: 1.3272 - acc: 0.5013 - val_loss: 1.5126 - val_acc: 0.4280\n",
      "Epoch 12/20\n",
      "18000/18000 [==============================] - 27s 1ms/step - loss: 1.3029 - acc: 0.5164 - val_loss: 1.5220 - val_acc: 0.4225\n",
      "Epoch 13/20\n",
      "18000/18000 [==============================] - 27s 1ms/step - loss: 1.2740 - acc: 0.5263 - val_loss: 1.5328 - val_acc: 0.4170\n",
      "Epoch 14/20\n",
      "18000/18000 [==============================] - 27s 1ms/step - loss: 1.2540 - acc: 0.5373 - val_loss: 1.5432 - val_acc: 0.4275\n",
      "Epoch 15/20\n",
      "18000/18000 [==============================] - 27s 1ms/step - loss: 1.2325 - acc: 0.5398 - val_loss: 1.5617 - val_acc: 0.4115\n",
      "Epoch 16/20\n",
      "18000/18000 [==============================] - 27s 1ms/step - loss: 1.2029 - acc: 0.5548 - val_loss: 1.5719 - val_acc: 0.4215\n",
      "Epoch 17/20\n",
      "18000/18000 [==============================] - 27s 1ms/step - loss: 1.1851 - acc: 0.5617 - val_loss: 1.5891 - val_acc: 0.4260\n",
      "Epoch 18/20\n",
      "18000/18000 [==============================] - 27s 1ms/step - loss: 1.1595 - acc: 0.5707 - val_loss: 1.6007 - val_acc: 0.4165\n",
      "Epoch 19/20\n",
      "18000/18000 [==============================] - 27s 1ms/step - loss: 1.1374 - acc: 0.5766 - val_loss: 1.6299 - val_acc: 0.4160\n",
      "Epoch 20/20\n",
      "18000/18000 [==============================] - 27s 1ms/step - loss: 1.1161 - acc: 0.5911 - val_loss: 1.6439 - val_acc: 0.4100\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(num_words_m, 300,  input_length=100, weights= [embedding_matrix_m], trainable=False))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(8, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "fit = model.fit(train_m, y_train_m, epochs=20, batch_size=batch_size,validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.6258691785812378, 0.426]\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(test_m, y_test_m, verbose=0)\n",
    "print(scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
